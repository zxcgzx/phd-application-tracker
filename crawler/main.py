#!/usr/bin/env python3
"""
ç”³è¯·åšå£«è®°å½• - çˆ¬è™«ä¸»ç¨‹åº

ç”¨æ³•:
    python main.py                    # çˆ¬å–æ‰€æœ‰å¯ç”¨çš„å­¦æ ¡
    python main.py --url <URL>        # çˆ¬å–æŒ‡å®šURL
    python main.py --dry-run          # æµ‹è¯•æ¨¡å¼ï¼ˆä¸å†™å…¥æ•°æ®åº“ï¼‰
"""

import os
import sys
import argparse
import yaml
from datetime import datetime
from typing import List, Dict, Any
from dotenv import load_dotenv

from scrapers import TwoLevelScraper
from database import SupabaseSync


def load_config(config_path: str = 'config.yaml') -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def create_scraper(university_config: Dict[str, Any], global_settings: Dict[str, Any]):
    """
    æ ¹æ®é…ç½®åˆ›å»ºçˆ¬è™«å®ä¾‹

    Args:
        university_config: å­¦æ ¡é…ç½®
        global_settings: å…¨å±€è®¾ç½®

    Returns:
        çˆ¬è™«å®ä¾‹
    """
    # åˆå¹¶å…¨å±€è®¾ç½®
    config = {**university_config, 'settings': global_settings}

    scraper_type = university_config.get('scraper_type', 'two_level')

    if scraper_type == 'two_level':
        return TwoLevelScraper(config)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„çˆ¬è™«ç±»å‹: {scraper_type}")


def crawl_university(
    university_config: Dict[str, Any],
    global_settings: Dict[str, Any],
    dry_run: bool = False
) -> Dict[str, Any]:
    """
    çˆ¬å–å•ä¸ªå­¦æ ¡

    Args:
        university_config: å­¦æ ¡é…ç½®
        global_settings: å…¨å±€è®¾ç½®
        dry_run: æ˜¯å¦ä¸ºæµ‹è¯•æ¨¡å¼

    Returns:
        çˆ¬å–ç»“æœç»Ÿè®¡
    """
    started_at = datetime.utcnow()
    result = {
        'university': university_config['name'],
        'status': 'success',
        'professors_found': 0,
        'new': 0,
        'updated': 0,
        'unchanged': 0,
        'error': None,
        'started_at': started_at.isoformat()
    }

    try:
        # åˆ›å»ºçˆ¬è™«
        scraper = create_scraper(university_config, global_settings)

        # æ‰§è¡Œçˆ¬å–
        professors = scraper.scrape()
        result['professors_found'] = len(professors)

        if not dry_run:
            # åŒæ­¥åˆ°æ•°æ®åº“
            db = SupabaseSync()

            # åŒæ­¥å­¦æ ¡ä¿¡æ¯
            university_id = db.sync_university({
                'name': university_config['name'],
                'url': university_config['url'],
                'scraper_type': university_config.get('scraper_type'),
                'list_page': university_config.get('list_page'),
                'detail_page': university_config.get('detail_page')
            })

            # åŒæ­¥å¯¼å¸ˆä¿¡æ¯
            sync_stats = db.sync_professors(university_id, professors)
            result.update(sync_stats)

            # æ›´æ–°å­¦æ ¡ç»Ÿè®¡
            db.update_university_stats(
                university_id,
                'success',
                result['professors_found']
            )

            # è®°å½•æ—¥å¿—
            db.log_crawl(university_id, 'success', result)

            print(f"\nâœ… åŒæ­¥å®Œæˆ:")
            print(f"   æ–°å¢: {sync_stats['new']} ä½")
            print(f"   æ›´æ–°: {sync_stats['updated']} ä½")
            print(f"   æœªå˜åŒ–: {sync_stats['unchanged']} ä½")
        else:
            print(f"\nğŸ§ª æµ‹è¯•æ¨¡å¼: æ‰¾åˆ° {len(professors)} ä½å¯¼å¸ˆï¼ˆæœªå†™å…¥æ•°æ®åº“ï¼‰")

            # æ˜¾ç¤ºå‰3ä½å¯¼å¸ˆçš„ä¿¡æ¯ä½œä¸ºç¤ºä¾‹
            for i, prof in enumerate(professors[:3], 1):
                print(f"\nç¤ºä¾‹å¯¼å¸ˆ {i}:")
                print(f"  å§“å: {prof.get('name')}")
                print(f"  èŒç§°: {prof.get('title', 'æœªçŸ¥')}")
                print(f"  é‚®ç®±: {prof.get('email', 'æœªæ‰¾åˆ°')}")
                print(f"  ç ”ç©¶æ–¹å‘: {', '.join(prof.get('research_areas', ['æœªæ‰¾åˆ°']))}")

    except Exception as e:
        result['status'] = 'failed'
        result['error'] = str(e)
        print(f"\nâŒ çˆ¬å–å¤±è´¥: {str(e)}")

        if not dry_run:
            # è®°å½•å¤±è´¥æ—¥å¿—
            try:
                db = SupabaseSync()
                university_id = db.sync_university({
                    'name': university_config['name'],
                    'url': university_config['url']
                })
                db.log_crawl(university_id, 'failed', result, str(e))
            except:
                pass

    return result


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç”³è¯·åšå£«è®°å½• - çˆ¬è™«ç¨‹åº')
    parser.add_argument('--url', help='æŒ‡å®šè¦çˆ¬å–çš„å­¦æ ¡URL')
    parser.add_argument('--dry-run', action='store_true', help='æµ‹è¯•æ¨¡å¼ï¼ˆä¸å†™å…¥æ•°æ®åº“ï¼‰')
    parser.add_argument('--config', default='config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    args = parser.parse_args()

    # åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv()

    # åŠ è½½é…ç½®
    try:
        config = load_config(args.config)
    except FileNotFoundError:
        print(f"âŒ é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {args.config}")
        sys.exit(1)

    universities = config.get('universities', [])
    global_settings = config.get('settings', {})

    # è¿‡æ»¤è¦çˆ¬å–çš„å­¦æ ¡
    if args.url:
        universities = [u for u in universities if u['url'] == args.url]
        if not universities:
            print(f"âŒ æœªæ‰¾åˆ°URLå¯¹åº”çš„å­¦æ ¡é…ç½®: {args.url}")
            sys.exit(1)
    else:
        # åªçˆ¬å–å¯ç”¨çš„å­¦æ ¡
        universities = [u for u in universities if u.get('enabled', True)]

    if not universities:
        print("âŒ æ²¡æœ‰è¦çˆ¬å–çš„å­¦æ ¡")
        sys.exit(1)

    # æ˜¾ç¤ºæ¨¡å¼
    mode = "ğŸ§ª æµ‹è¯•æ¨¡å¼" if args.dry_run else "ğŸš€ æ­£å¸¸æ¨¡å¼"
    print(f"\n{mode}")
    print("=" * 60)
    print(f"è®¡åˆ’çˆ¬å– {len(universities)} æ‰€å­¦æ ¡\n")

    # é€ä¸ªçˆ¬å–
    results = []
    for i, uni_config in enumerate(universities, 1):
        print(f"\n[{i}/{len(universities)}] {uni_config['name']}")
        print("-" * 60)

        result = crawl_university(uni_config, global_settings, args.dry_run)
        results.append(result)

    # æ±‡æ€»ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("ğŸ“Š çˆ¬å–æ±‡æ€»")
    print("=" * 60)

    total_found = sum(r['professors_found'] for r in results)
    total_new = sum(r.get('new', 0) for r in results)
    total_updated = sum(r.get('updated', 0) for r in results)
    success_count = sum(1 for r in results if r['status'] == 'success')

    print(f"æˆåŠŸ: {success_count}/{len(results)} æ‰€å­¦æ ¡")
    print(f"å‘ç°å¯¼å¸ˆ: {total_found} ä½")

    if not args.dry_run:
        print(f"æ–°å¢: {total_new} ä½")
        print(f"æ›´æ–°: {total_updated} ä½")

    # æ˜¾ç¤ºå¤±è´¥çš„å­¦æ ¡
    failed = [r for r in results if r['status'] == 'failed']
    if failed:
        print("\nâŒ å¤±è´¥çš„å­¦æ ¡:")
        for r in failed:
            print(f"  - {r['university']}: {r['error']}")

    print("\nâœ¨ å®Œæˆ!\n")


if __name__ == '__main__':
    main()
