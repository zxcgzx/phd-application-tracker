#!/usr/bin/env python3
"""
ç®€å•çš„çˆ¬è™«æµ‹è¯•è„šæœ¬ - æ— éœ€å®‰è£…ä¾èµ–
"""

import urllib.request
import re
from html.parser import HTMLParser


class SimpleHTMLParser(HTMLParser):
    """ç®€å•çš„HTMLè§£æžå™¨"""

    def __init__(self):
        super().__init__()
        self.links = []
        self.current_tag = None
        self.current_attrs = {}

    def handle_starttag(self, tag, attrs):
        self.current_tag = tag
        self.current_attrs = dict(attrs)

        # å¦‚æžœæ˜¯é“¾æŽ¥,è®°å½•ä¸‹æ¥
        if tag == 'a' and 'href' in self.current_attrs:
            self.links.append({
                'href': self.current_attrs['href'],
                'text': ''
            })

    def handle_data(self, data):
        # å¦‚æžœåœ¨é“¾æŽ¥å†…,è®°å½•æ–‡æœ¬
        if self.current_tag == 'a' and self.links:
            self.links[-1]['text'] += data.strip()


def test_fetch_page():
    """æµ‹è¯•1: èƒ½å¦è®¿é—®åŒ—ç†å·¥ç½‘ç«™"""
    url = "https://ac.bit.edu.cn/szdw/dsmd/bssds/index.htm"

    print("=" * 60)
    print("æµ‹è¯•1: è®¿é—®åŒ—ç†å·¥å¯¼å¸ˆåˆ—è¡¨é¡µ")
    print("=" * 60)
    print(f"URL: {url}\n")

    try:
        # åˆ›å»ºè¯·æ±‚
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        req = urllib.request.Request(url, headers=headers)

        # å‘é€è¯·æ±‚
        print("æ­£åœ¨å‘é€è¯·æ±‚...")
        with urllib.request.urlopen(req, timeout=10) as response:
            html = response.read().decode('utf-8', errors='ignore')

        print(f"âœ… æˆåŠŸ! å“åº”é•¿åº¦: {len(html)} å­—ç¬¦\n")

        # ä¿å­˜HTMLç”¨äºŽè°ƒè¯•
        with open('/tmp/bit_list_page.html', 'w', encoding='utf-8') as f:
            f.write(html)
        print("âœ… HTMLå·²ä¿å­˜åˆ°: /tmp/bit_list_page.html\n")

        return html

    except Exception as e:
        print(f"âŒ å¤±è´¥: {e}\n")
        return None


def test_parse_links(html):
    """æµ‹è¯•2: è§£æžå¯¼å¸ˆé“¾æŽ¥"""

    print("=" * 60)
    print("æµ‹è¯•2: è§£æžå¯¼å¸ˆé“¾æŽ¥")
    print("=" * 60)

    try:
        parser = SimpleHTMLParser()
        parser.feed(html)

        # ç­›é€‰å‡ºå¯èƒ½æ˜¯å¯¼å¸ˆè¯¦æƒ…é¡µçš„é“¾æŽ¥
        professor_links = []
        for link in parser.links:
            href = link['href']
            text = link['text']

            # å‡è®¾å¯¼å¸ˆé¡µé¢åŒ…å«æŸäº›ç‰¹å¾
            if text and len(text) > 0 and len(text) < 20:  # å§“åé€šå¸¸å¾ˆçŸ­
                # æŽ’é™¤å¯¼èˆªé“¾æŽ¥
                if 'index' not in href.lower() and 'list' not in href.lower():
                    professor_links.append(link)

        print(f"æ‰¾åˆ° {len(professor_links)} ä¸ªå¯èƒ½çš„å¯¼å¸ˆé“¾æŽ¥\n")

        # æ˜¾ç¤ºå‰5ä¸ª
        print("å‰5ä¸ªé“¾æŽ¥:")
        for i, link in enumerate(professor_links[:5], 1):
            print(f"  {i}. {link['text']}")
            print(f"     URL: {link['href']}\n")

        return professor_links

    except Exception as e:
        print(f"âŒ è§£æžå¤±è´¥: {e}\n")
        return []


def test_extract_emails(html):
    """æµ‹è¯•3: æå–é‚®ç®±"""

    print("=" * 60)
    print("æµ‹è¯•3: æå–é‚®ç®±åœ°å€")
    print("=" * 60)

    # é‚®ç®±æ­£åˆ™
    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
    emails = re.findall(email_pattern, html)

    # åŽ»é‡
    emails = list(set(emails))

    print(f"æ‰¾åˆ° {len(emails)} ä¸ªé‚®ç®±åœ°å€\n")

    if emails:
        print("ç¤ºä¾‹é‚®ç®±:")
        for email in emails[:5]:
            print(f"  - {email}")

    print()
    return emails


def analyze_html_structure(html):
    """æµ‹è¯•4: åˆ†æžHTMLç»“æž„"""

    print("=" * 60)
    print("æµ‹è¯•4: HTMLç»“æž„åˆ†æž")
    print("=" * 60)

    # ç»Ÿè®¡å¸¸è§æ ‡ç­¾
    tags = {
        '<ul': html.count('<ul'),
        '<li': html.count('<li'),
        '<div': html.count('<div'),
        '<a': html.count('<a'),
        '<span': html.count('<span'),
    }

    print("æ ‡ç­¾ç»Ÿè®¡:")
    for tag, count in tags.items():
        print(f"  {tag}: {count}")

    print()

    # æŸ¥æ‰¾åŒ…å«"æ•™æŽˆ"çš„å†…å®¹
    if 'æ•™æŽˆ' in html or 'å‰¯æ•™æŽˆ' in html:
        print("âœ… é¡µé¢åŒ…å«'æ•™æŽˆ'/'å‰¯æ•™æŽˆ'å…³é”®è¯")
    else:
        print("âš ï¸  é¡µé¢ä¸åŒ…å«'æ•™æŽˆ'å…³é”®è¯")

    # æŸ¥æ‰¾åˆ—è¡¨ç»“æž„
    if '<ul' in html and '<li' in html:
        print("âœ… é¡µé¢åŒ…å«åˆ—è¡¨ç»“æž„ (ul/li)")

    print()


def main():
    """ä¸»æµ‹è¯•æµç¨‹"""

    print("\n" + "=" * 60)
    print("åŒ—äº¬ç†å·¥å¤§å­¦çˆ¬è™«æµ‹è¯•")
    print("=" * 60 + "\n")

    # æµ‹è¯•1: è®¿é—®é¡µé¢
    html = test_fetch_page()
    if not html:
        print("âŒ æ— æ³•è®¿é—®é¡µé¢,æµ‹è¯•ç»ˆæ­¢")
        return

    # æµ‹è¯•2: è§£æžé“¾æŽ¥
    links = test_parse_links(html)

    # æµ‹è¯•3: æå–é‚®ç®±
    emails = test_extract_emails(html)

    # æµ‹è¯•4: åˆ†æžç»“æž„
    analyze_html_structure(html)

    # æ€»ç»“
    print("=" * 60)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    print(f"âœ“ é¡µé¢å¯è®¿é—®: æ˜¯")
    print(f"âœ“ å¯¼å¸ˆé“¾æŽ¥æ•°: {len(links)}")
    print(f"âœ“ é‚®ç®±åœ°å€æ•°: {len(emails)}")
    print(f"âœ“ HTMLæ–‡ä»¶: /tmp/bit_list_page.html")
    print()

    print("ðŸ’¡ ä¸‹ä¸€æ­¥:")
    print("  1. æ£€æŸ¥ /tmp/bit_list_page.html ç¡®è®¤HTMLç»“æž„")
    print("  2. æ ¹æ®å®žé™…ç»“æž„è°ƒæ•´ crawler/config.yaml ä¸­çš„é€‰æ‹©å™¨")
    print("  3. å¦‚æžœé“¾æŽ¥æ•°ä¸º0,éœ€è¦æ›´æ–° list_page.container_selector")
    print()


if __name__ == '__main__':
    main()
